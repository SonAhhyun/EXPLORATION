{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blocked-square",
   "metadata": {},
   "source": [
    "<h1> 트랜스포머로 만드는 대화형 챗봇 </h1>\n",
    "\n",
    "## 들어가며\n",
    "\n",
    "### 대화형 챗봇이란?\n",
    "\n",
    "챗봇에는 인간과 자연어로 대화를 주고받는 대화형 챗봇 이외에도, 정해진 트리형 메뉴 구조를 따라가는 트리형(버튼) 챗봇, 추천형 챗봇, 시나리오형 챗봇이 있고, 이들을 결합한 결합형 챗봇이 있다고 합니다. 대화형을 제외하면 사실상 챗봇은 대화형 UX를 가졌지만 본질적으로는 검색엔진이거나, 혹은 음성 ARS를 대화형 UX에 옮겨놓은 것이다.\n",
    "\n",
    "챗봇이 가지는 한계는 명확한데, 규칙 기반으로 구현된 챗봇은 사전에 정해진 말만 알아듣고 반응할 수 있는데, 진정한 챗봇의 가치는 사용자가 어떤 말을 하더라도 알아듣고 적절히 대응할 수 있는 자유도에 있기 때문입니다. 딥러닝을 통한 자연어처리 기술이 새로운 가능성을 보여주자, 사람들은 챗봇이 가져올 혁명적인 변화에 새삼 기대감을 가지기 시작했다.\n",
    "\n",
    "## 챗봇과 딥러닝\n",
    "\n",
    "인간보다 정확하게 퀴즈를 풀어내는 BERT, ALBERT 등은 모두 트랜스포머(Transformer)라는 모델을 활용하여 pretrain을 적용한 것이다. 트랜스포머 이전에도 LSTM 등 RNN 기반의 딥러닝 모델, 그리고 이를 인코더-디코더 구조로 엮은 seq2seq 모델 등을 활용하여 챗봇 제작을 시도해 왔으나 2017년에 발표된 트랜스포머는 병렬처리에 불리한 LSTM에 비해 훨씬 뛰어난 처리 속도를 보이면서도 LSTM 등 RNN 모델이 가지는 장기 의존성에 강건한 특징 때문에 매우 긴 길이의 문장을 처리하는 데 유리하다는 좋은 특징을 보여주었고, 이후 자연어처리 분야의 혁신을 가져온 발판이 되어 주었다.\n",
    "\n",
    "그래서 트랜스포머 모델을 기반으로 한 인코더-디코더 구조를 바탕으로 챗봇을 제작해 보려고 한다. 이미 인코더-디코더 구조에 대해서는 seq2seq, AutoEncoder, GAN 등에서 여러 번 경험해 본 내용을 바탕으로 seq2seq 모델을 기반으로 하는 번역기를 생각해 보자. 영어를 한국어로 번역하는 모델은 영어 문장을 인코더로 해석하여 나온 벡터를 디코더의 인풋(input)으로 삼아 디코더에서 한국어 문장을 생성하게 한다. 그런 것처럼 사용자의 입력 문장을 인코더로 해석하고, 적절한 답변 문장을 디코더가 생성하도록 구성할 수 있을 것이다.\n",
    "\n",
    "물론 더욱 좋은 성능을 내기 위해서는 엄청나게 많은 코퍼스로 학습시킨 pretrained model을 활용하는 것이 필요하다. 오늘 우리가 다룰 데이터의 규모로 충분히 스마트한 챗봇을 만들기는 어려울 것입니다. 그러나 그런 챗봇을 만들 수 있는 모델의 기본 구조를 알아보는 데는 충분히 도움이 될 것이다.\n",
    "\n",
    "### 학습 목표\n",
    "\n",
    "---\n",
    "\n",
    "- 트랜스포머의 인코더 디코더 구조 이해하기\n",
    "- 내부 단어 토크나이저 사용하기\n",
    "- 셀프 어텐션 이해하기\n",
    "- 한국어에도 적용해보기\n",
    "\n",
    "\n",
    "$ mkdir -p ~/aiffel/EXPLORATION/15/songys_chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-plant",
   "metadata": {},
   "source": [
    "## 트랜스포머와 인코더 디코더\n",
    "\n",
    "### 인코더와 디코더 구조 되짚어보기\n",
    "\n",
    "번역기를 만드는 데 사용한 대표적인 모델인 인코더와 디코더 구조를 살펴보면..\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_UcFQAjh.max-800x600.png?raw=true)\n",
    "\n",
    "번역기는 인코더와 디코더 두 가지 아키텍처로 구성돼 있으며, 인코더에 입력 문장이 들어가고, 디코더는 이에 상응하는 출력 문장을 생성합니다. 그리고 이를 훈련한다는 것은 결국 입력 문장과 출력 문장 두 가지 병렬 구조로 구성된 데이터셋을 훈련한다는 의미이다.\n",
    "\n",
    "### 훈련 데이터셋의 구성(번역)\n",
    "\n",
    "입력 문장 : '저는 학생입니다.'\n",
    "출력 문장 : 'I am a student'\n",
    "\n",
    "이런 병렬적으로 구성된 데이터셋을 인코더와 디코더로 학습하는 경우는 사실 번역기 뿐만 아니라, 질문에 대해서 대답을 하도록 구성된 데이터셋을 인코더와 디코더 구조로 학습한다면, 주어진 질문에 답변할 수 있는 챗봇 또한 만들 수 있다.\n",
    "\n",
    "### 훈련 데이터셋의 구성(질문-답변)\n",
    "\n",
    "입력 문장 : '오늘의 날씨는 어때?'\n",
    "출력 문장 : '오늘은 매우 화창한 날씨야'\n",
    "\n",
    "### 트랜스포머의 인코더와 디코더\n",
    "\n",
    "트랜스포머 또한 번역기와 마찬가지로 기본적으로 인코더와 디코더 구성을 가지고 있어서, 입력 문장을 넣으면 출력 문장을 제공한다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_1_kxflIxg.max-800x600.png?raw=true)\n",
    "위의 블랙박스로 가려져 있는 트랜스포머의 내부 구조를 열어보면 아래와 같다!\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_2_EnQyi4S.max-800x600.png?raw=true)\n",
    "초록색 색깔의 도형을 인코더 층(Encoder layer), 핑크색 색깔의 도형을 디코더(Decoder layer)라고 하였을 때, 입력 문장은 누적해 쌓아 올린 인코더의 층을 통해서 정보를 뽑아내고, 디코더는 누적해 쌓아 올린 디코더의 층을 통해서 출력 문장의 단어를 하나씩 만들어가는 구조를 갖고 있다.\n",
    "\n",
    "그리고 그 내부를 조금 더 확대해 보면 아래와 같이 톱니바퀴처럼 맞물려 돌아가는 여러 가지 부품들로 구성돼 있다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_3_ddZedfW.max-800x600.png?raw=true)\n",
    "위의 그림에서 적힌 모듈들을 하나씩 정리해 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-donna",
   "metadata": {},
   "source": [
    "## 트랜스포머의 입력 이해하기\n",
    "\n",
    "필요한 패키지를 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "challenging-deputy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-receipt",
   "metadata": {},
   "source": [
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_4_fuzN6PD.png?raw=true)\n",
    "\n",
    "많은 자연어 처리 모델들은 텍스트 문장을 입력으로 받기 위해 단어를 임베딩 벡터로 변환하는 벡터화 과정을 거치며 트랜스포머 또한 그 점에서는 다른 모델들과 다르지 않지만 트랜스포머 모델의 입력 데이터 처리에는 RNN 계열의 모델들과 다른 점이 한 가지 있는데, 바로 임베딩 벡터에 어떤 값을 더해준 뒤에 입력으로 사용한다는 점이다. 그 값은 바로 위 그림에서의 포지셔널 인코딩(positional Encoding)에 해당하는 부분이다.\n",
    "\n",
    "위 그림에서 인코더의 입력 부분을 조금 더 확대해 본다면 이런 그림이 나온다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_5_kH52kQN.png?raw=true)\n",
    "\n",
    "이렇게 해주는 이유는 트랜스포머는 입력을 받을 때, 문장에 있는 단어들을 1개씩 순차적으로 받는 것이 아니라, 문장에 있는 모든 단어를 한꺼번에 입력으로 받기 때문이다. 트랜스포머가 RNN과 결정적으로 다른 점이 바로 이 부분인데 RNN에는 어차피 문장을 구성하는 단어들이 어순대로 모델에 입력되므로, 모델에게 따로 어순 정보를 알려줄 필요가 없으나 문장에 있는 모든 단어를 한꺼번에 문장 단위로 입력받는 트랜스포머는 자칫 'I ate lunch'와 'lunch ate I'를 구분할 수 없을지도 모른다. 그래서 같은 단어라도 그 단어가 문장의 몇 번째 어순으로 입력되었는지를 모델에 추가로 알려 주기 위해, 단어의 임베딩 벡터에다가 위치 정보를 가진 벡터(Positional Encoding) 값을 더해서 모델의 입력으로 받는다\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_6_DyxB6Ax.png?raw=true)\n",
    "포지셔널 인코딩의 벡터값은 위의 수식에 의해서 정해집니다. 사인 함수와 코사인 함수의 그래프를 상기해보면 요동치는 값의 형태를 생각해 볼 수 있는데, 트랜스포머는 사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해줌으로써 단어의 순서 정보를 더하여 준다.\n",
    "\n",
    "위의 두 함수에는 pos,i,d<sub>model</sub> 등 생소한 변수들이 있는데, 위의 함수를 이해하기 위해서는 위에서 본 임베딩 벡터와 포지셔널 인코딩의 덧셈은 사실 임베딩 벡터가 모여 만들어진 문장 벡터 행렬과 포지셔널 인코딩 행렬의 덧셈 연산을 통해 이루어진다는 점을 이해해야 한다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_7_3Rneu0P.png?raw=true)\n",
    "\n",
    "d<sub>model</sub>은 임베딩 벡터의 차원을 의미하고 있고, pospos는 입력 문장에서의 임베딩 벡터의 위치를 나타내며, ii는 임베딩 벡터 내의 차원의 인덱스를 의미하며, 이렇게 임베딩 행렬과 포지셔널 행렬이라는 두 행렬을 더함으로써 각 단어 벡터에 위치 정보를 더해주게 되는 것이다\n",
    " \n",
    "포지셔널 행렬을 직접 구현해서 눈으로 확인해 보면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handled-stable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-marshall",
   "metadata": {},
   "source": [
    "행의 크기가 50, 열의 크기가 512인 행렬을 그려보면... 최대 문장의 길이가 50이고 워드 임베딩 차원을 512로 하는 모델의 입력 벡터 모양이 이와 같을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pregnant-causing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_pos_encoding = PositionalEncoding(50, 512)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-customer",
   "metadata": {},
   "source": [
    " 논문에서 제시된 그림에서는 다음과 같이 포지셔널 인코딩을 표현해보면...\n",
    " \n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_9_l58gVWT.max-800x600.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-familiar",
   "metadata": {},
   "source": [
    "## 어텐션? 어텐션!\n",
    "\n",
    "트랜스포머의 인코더와 디코더에서 사용하고 있는 개념인 어텐션에 대해서 알아보자!\n",
    "\n",
    "### 어텐션이란?\n",
    "\n",
    "어텐션 메커니즘을 그림으로 표현한다면 아래와 같이 표현할 수 있다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_10_AaCfqrY.png?raw=true)\n",
    "\n",
    "어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구한다. 그리고 구해낸 이 유사도를 키(Key)와 맵핑되어있는 각각의 '값(Value)'에 반영해 주고 유사도가 반영된 '값(Value)'을 모두 더해서 뭉쳐주면 이를 최종 결과인 어텐션 값(Attention Value) 라고 한다.\n",
    "\n",
    "### 트랜스포머에서 사용된 어텐션\n",
    "\n",
    "트랜스포머는 총 세 가지의 어텐션을 사용하는데..\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_11_tFFhFjx.png?raw=true)\n",
    "\n",
    "첫 번째 그림인 인코더 셀프 어텐션은 인코더에서 이루어지고,\n",
    "\n",
    "두 번째 그림인 디코더 셀프 어텐션은 디코더에서 이루어지며,\n",
    "\n",
    "세 번째 그림인 인코더-디코더 어텐션 또한 디코더에서 이루어집니다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_12_SIe2V15.png?raw=true)\n",
    "\n",
    "위 그림은 트랜스포머의 전체적인 아키텍처에서 각 어텐션이 위치한 곳을 보여준다.\n",
    "\n",
    "트랜스포머의 어텐션 함수에 사용되는 쿼리(Query), 키(Key), 밸류(Value) 는 기본적으로 '단어 (정보를 함축한) 벡터' 이다.\n",
    "\n",
    "단, 여기서 '단어 벡터' 란 초기 입력으로 사용되었던 임베딩 벡터가 아니고, 트랜스포머의 여러 연산을 거친 후의 단어 벡터다.\n",
    "\n",
    "그럼 위 세 가지 어텐션이 하는 일을 조금 더 자세히 알아보면...\n",
    "\n",
    "- 인코더 셀프 어텐션 : 인코더의 입력으로 들어간 문장 내 단어들이 서로 유사도를 구한다.\n",
    "- 디코더 셀프 어텐션 : 단어를 1개씩 생성하는 디코더가 이미 생성된 앞 단어들과의 유사도를 구한다.\n",
    "- 인코더-디코더 어텐션 : 디코더가 잘! 예측하기 위해서 인코더에 입력된 단어들과 유사도를 구한다.\n",
    "\n",
    "### 셀프 어텐션(Self Attention)\n",
    "\n",
    "셀프 어텐션이란 유사도를 구하는 대상이 다른 문장의 단어가 아니라 현재 문장 내의 단어들이 서로 유사도를 구하는 경우를 말하는데 가령, 위에서 언급한 인코더-디코더 어텐션은 서로 다른 단어 목록(인코더 내 단어와 디코더 내 단어) 사이에서 유사도를 구하기에 셀프 어텐션이 아니다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_13_hjMyZwL.png?raw=true)\n",
    "위의 그림은 구글 AI 블로그 포스트에서 가져왔는데, 위의 예시 문장을 번역하면 '그 동물은 길을 건너지 않았다. 왜냐하면 그것은 너무 피곤하였기 때문이다.' 라는 의미가 됩니다. 그런데 여기서 그것(it) 에 해당하는 것은 과연 길(street) 일까요? 동물(animal) 일까요?\n",
    "\n",
    "우리는 동물이라는 것을 쉽게 알 수 있지만, 기계는 그렇지 않다. 하지만 셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구하여 그것(it) 이 동물(animal) 과 연관되었을 확률이 높다는 것을 찾아낸다. 그러면 유사도는 어떻게 구할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-azerbaijan",
   "metadata": {},
   "source": [
    "## 스케일드 닷 프로덕트 어텐션\n",
    "\n",
    "\n",
    "트랜스포머에서는 어텐션 값을 구하는 방법으로 아래와 같은 수식을 사용했다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_14_pUfIgKn.png?raw=true)\n",
    "\n",
    "Q,K,V는 각각 쿼리(Query), 키(Key), 값(Value)를 나타냅니다.\n",
    "\n",
    "앞서 언급했던 어텐션 함수의 정의와 결괏값을 다시 상기해보면..\n",
    "\n",
    "어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구한다. 그리고 구해낸 이 유사도를 키와 맵핑되어있는 각각의 '값(Value)'에 반영해 준다. 그리고 유사도가 반영된 '값(Value)'을 모두 더해서 뭉쳐주면 이를 최종 결과인 어텐션 값(Attention Value) 라고 한다.\n",
    "\n",
    "위 정의와 아래 내용 세 가지만 기억하면 수식을 그림으로 정리할 수 있다.\n",
    "\n",
    "Q, K, V는 단어 벡터를 행으로 하는 문장 행렬이다.\n",
    "벡터의 내적(dot product) 은 벡터의 유사도를 의미한다.\n",
    "특정 값을 분모로 사용하는 것은 값의 크기를 조절하는 스케일링(Scaling)을 위함이다.\n",
    "우선 Q와 K의 전치 행렬을 곱하는 것을 그림으로 표현하면 다음과 같습니다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_15_pUfIgKn.png?raw=true)\n",
    "문장 행렬 Q와 문장 행렬 K를 곱하면 위의 그림과 같은 초록색 행렬을 얻을 수 있다.\n",
    "\n",
    "위 초록색 행렬이 의미하는 값은 무엇일까요? 예를 들어 'am' 행과 'student' 열의 값은 Q 행렬에 있던 'am' 벡터와 K 행렬에 있던 'student 벡터'의 내적값을 의미하며, 결국 각 단어 벡터의 유사도가 모두 기록된 유사도 행렬이 되는 것이다\n",
    "\n",
    "이 유사도 값을 스케일링 해주기 위해서 행렬 전체를 특정 값으로 나눠주고, 유사도를 0과 1사이의 값으로 Normalize해주기 위해서 소프트맥스 함수를 사용한다. 여기까지가 Q와 K의 유사도를 구하는 과정이라고 볼 수 있으며, 여기에 문장 행렬 V와 곱하면 어텐션 값(Attention Value) 를 얻는다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_16_neA52rZ.png?raw=true)\n",
    "결국 이를 모두 하나의 그림으로 표현하면 위와 같다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_14_pUfIgKn.png?raw=true)\n",
    "\n",
    "이 수식은 내적(dot product)을 통해 단어 벡터 간 유사도를 구한 후에, 특정 값을 분모로 나눠주는 방식으로 Q와 K의 유사도를 구하였다고 하여 스케일드 닷 프로덕트 어텐션(Scaled Dot Product Attention) 이라고 한다. 유사도를 구하는 방법이 스케일드 닷 프로덕트(scaled dot product)였기 때문에 이런 이름이 붙는다.\n",
    "\n",
    "만약에 분모에 특정 값을 나눠주는 부분을 사용하지 않았다면 어텐션의 이름은 닷 프로덕트 어텐션(dot product attention) 이라고 부른다.\n",
    "\n",
    "### 구현하기\n",
    "\n",
    "스케일드 닷 프로덕트 어텐션 함수를 구현해면.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "front-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-justice",
   "metadata": {},
   "source": [
    "## 머리가 여러 개인 어텐션\n",
    "\n",
    "### 병렬로 어텐션 수행하기\n",
    "\n",
    "트랜스포머에서 num_heads라는 변수는 기계가 몇 개의 똑똑한 머리를 사용할지, 다시 말해 병렬적으로 몇 개의 어텐션 연산을 수행할지를 결정하는 하이퍼파라미터이다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_18_nnOTx9p.png?raw=true)\n",
    "\n",
    "앞서 포지셔널 인코딩에서 d_model은 임베딩 벡터의 차원이라고 언급한 바 있습니다. 결국 트랜스포머의 초기 입력인 문장 행렬의 크기는 문장의 길이를 행으로, d_model을 열의 크기로 가진다.\n",
    "\n",
    "트랜스포머는 이렇게 입력된 문장 행렬을 num_heads의 수만큼 쪼개서 어텐션을 수행하고, 이렇게 얻은 num_heads의 개수만큼의 어텐션 값 행렬을 다시 하나로 concatenate한다.\n",
    "\n",
    "위의 그림은 num_heads가 8개인 경우인데, 다시 concatenate하면서 열의 크기가 d_model이 된다.\n",
    "\n",
    "\n",
    "### 멀티-헤드 어텐션\n",
    "\n",
    "이렇게 병렬로 어텐션을 수행하면 얻을 수 있는 효과는 무엇일까?\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_19_FwmaA3q.png?raw=true)\n",
    "\n",
    "위 그림은 num_heads의 값이 8일 때, 병렬로 수행되는 어텐션이 서로 다른 셀프 어텐션 결과를 얻을 수 있음을 보여준다. 다시 말해 8개의 머리는 각각 다른 관점에서 어텐션을 수행하므로 한 번의 어텐션만 수행했다면 놓칠 수도 있던 정보를 캐치할 수 있는데... 예를 들어 위 그림에서라면 it_이라는 토큰이 animal_과 유사하다고 보는 관점과 street_과 유사하다고 보는 관점이 한꺼번에 모두 표현 가능하다는 뜻이다.\n",
    "\n",
    "이와 같이 어텐션을 병렬로 수행하는 것을 멀티 헤드 어텐션이라고 부르다.\n",
    "\n",
    "### 구현하기\n",
    "\n",
    "멀티 헤드 어텐션을 구현하면 다음과 같습니다.\n",
    "\n",
    "내부적으로는 스케일드 닷 프로덕트 어텐션 함수를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arctic-knowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-dependence",
   "metadata": {},
   "source": [
    "## 마스킹\n",
    "\n",
    "마스킹(Masking) 이란, 특정 값들을 가려서 실제 연산에 방해가 되지 않도록 하는 기법이다.\n",
    "\n",
    "### 패딩 마스킹(Padding Masking)\n",
    "첫 번째 마스킹은 패딩 토큰(Padding token)을 이용한 방법이다.\n",
    "\n",
    "자연어 처리에서 패딩(Padding)이란 어떤 개념일까?\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/1365906-20200410103623697-871078599.max-800x600.png?raw=true)\n",
    "패딩은 문장의 길이가 서로 다를 때, 모든 문장의 길이를 동일하게 해주는 과정에서 정해준 길이보다 짧은 문장의 경우에는 숫자 0을 채워서 문장의 길이를 맞춰주는 자연어 처리 전처리 방법이다.\n",
    "\n",
    "위 그림은 케라스의 pad_sequences()를 사용하여 패딩을 하는 과정을 시각화한 그림이다.\n",
    "\n",
    "그런데 사실 이렇게 주어진 숫자 0은 실제 의미가 있는 단어가 아니므로 실제 어텐션 등과 같은 연산에서는 제외할 필요가 있으며, 패딩 마스킹은 이를 위해 숫자 0인 위치를 체크한.\n",
    "\n",
    "다음은 패딩 마스킹을 구현한 함수다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "national-appendix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-personality",
   "metadata": {},
   "source": [
    "이 함수에 정수 시퀀스를 입력으로 하면, 이 함수는 숫자가 0인 부분을 체크한 벡터를 리턴한다.\n",
    "\n",
    "두 개의 정수 시퀀스를 입력으로 해보고, 각각 어떤 결과가 나오는지 확인해 보면.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "employed-dominant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 0. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(2, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-zimbabwe",
   "metadata": {},
   "source": [
    "두 정수 시퀀스에 대해서 각각 결과가 출력되는데, 오직 숫자가 0인 위치에서만 숫자 1이 나오고 숫자 0이 아닌 위치에서는 숫자 0인 벡터를 출력한다.\n",
    "\n",
    "### 룩 어헤드 마스킹(Look-ahead masking, 다음 단어 가리기)\n",
    "\n",
    "순환 신경망, RNN과 트랜스포머는 문장을 입력받을 때 입력받는 방법이 전혀 다르다.\n",
    "\n",
    "RNN은 step이라는 개념이 존재해서 각 step마다 단어가 순서대로 입력으로 들어가는 구조인 반면 트랜스포머의 경우에는 문장 행렬을 만들어 한 번에 행렬 형태로 입력으로 들어간다는 특징이 있고, 이 특징 때문에 추가적인 마스킹(Masking) 을 필요하다.\n",
    "\n",
    "### RNN\n",
    "RNN으로 다음 단어를 예측해가면서 문장을 생성해내는 과정을 보면..\n",
    "\n",
    "다시 말해 RNN으로 디코더를 구현했을 경우\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_20_NAntZnv.max-800x600.png?raw=true)\n",
    "\n",
    "RNN은 구조상으로 다음 단어를 만들어 갈 때, 자신보다 앞에 있는 단어들만 참고해서 다음 단어를 예측한다. 위의 그림을 참고로 각 단계에서 다음 단어 예측 과정을 서술하면 다음과 같다.\n",
    "\n",
    "### 첫 번째 step\n",
    "\n",
    "현재까지의 입력 : what → 출력 : is\n",
    "\n",
    "### 두 번째 step\n",
    "\n",
    "현재까지의 입력 : what is → 출력 : the\n",
    "\n",
    "### 세 번째 step\n",
    "\n",
    "현재까지의 입력 : what is the → 출력 problem\n",
    "\n",
    "### 트랜스포머\n",
    "\n",
    "트랜스포머의 경우, 전체 문장이 문장 행렬로 들어가기 때문에 위치와 상관없이 모든 단어를 참고해서 다음 단어를 예측할 수 있지만 우리가 원하는 것은 이전 단어들로부터 다음 단어를 예측하는 훈련을 제대로 하는 것이다. 따라서 이러한 문제를 해결하기 위해 자신보다 다음에 나올 단어를 참고하지 않도록 가리는 기법이 룩 어헤드 마스킹 기법이다.\n",
    "\n",
    "이 기법은 어텐션을 수행할 때, Query 단어 뒤에 나오는 Key 단어들에 대해서는 마스킹 한다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/max-800x600.png?raw=true)\n",
    "\n",
    "위의 그림에서 빨간색으로 색칠된 부분은 마스킹을 표현하고 있다. 빨간색은 실제 어텐션 연산에서 가리는 역할을 하여 어텐션 연산 시에 현재 단어를 기준으로 이전 단어들하고만 유사도를 구할 수 있다. 행을 Query, 열을 Key로 표현된 행렬임을 감안하고 천천히 행렬을 살펴보면...\n",
    "\n",
    "예를 들어 Query 단어가 '찾고'라고 한다면, 이 '찾고'라는 행에는 <s>, <나는>, <행복을>, <찾고>까지의 열만 보이고 그 뒤 열은 아예 빨간색으로 칠해져 있습니다. 즉, 유사도를 구할 수 없도록 해놓았다. 저 빨간색 부분을 마스킹 함수로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "consecutive-cooling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "narrow-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[1, 2, 3, 4, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-making",
   "metadata": {},
   "source": [
    "대각선의 형태로 숫자 1이 채워지는 것을 볼 수 있는데 이 마스킹과 패딩 마스킹은 별개이므로, 이 마스킹을 수행할 때 만약에 숫자 0인 단어가 있다면 이 또한 패딩 해야 한다. 그래서 create_look_ahead_mask() 함수는 내부적으로 앞서 구현한 패딩 마스크 함수도 호출하고 있다.\n",
    "\n",
    "숫자 0이 포함되었을 경우에도 테스트해 보면...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "addressed-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 0. 1. 1. 1.]\n",
      "   [1. 0. 0. 1. 1.]\n",
      "   [1. 0. 0. 0. 1.]\n",
      "   [1. 0. 0. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[0, 5, 1, 5, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-algebra",
   "metadata": {},
   "source": [
    "## 인코더\n",
    "\n",
    "\n",
    "### 인코더 층 만들기\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_21_Y7Cy8sm.max-800x600.png?raw=true)\n",
    "\n",
    "하나의 인코더 층은 크게 총 2개의 서브 층(sublayer)으로 나누어지며, 바로 셀프 어텐션과 피드 포워드 신경망이다. 셀프 어텐션은 멀티 헤드 어텐션으로 병렬적으로 이루어진다.\n",
    "\n",
    "두 개의 서브 층을 가지는 하나의 인코더 층을 구현하는 함수는 다음과 같다. 함수 내부적으로 첫 번째 서브 층과 두 번째 서브 층을 구현하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "short-packaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-tourist",
   "metadata": {},
   "source": [
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_22_teJgoCi.max-800x600.png?raw=true)\n",
    "\n",
    "### 인코더 층을 쌓아 인코더 만들기\n",
    "\n",
    "이렇게 구현한 인코더 층을 임베딩 층(Embedding layer) 과 포지셔널 인코딩(Positional Encoding) 을 연결하고, 사용자가 원하는 만큼 인코더 층을 쌓음으로써 트랜스포머의 인코더가 완성된다.\n",
    "\n",
    "인코더와 디코더 내부에서는 각 서브 층 이후에 훈련을 돕는 Layer Normalization이라는 테크닉이 사용되었는데, 위 그림에서는 Normalize라고 표시된 부분에 해당된다.\n",
    "\n",
    "트랜스포머는 하이퍼파라미터인 num_layers 개수의 인코더 층을 쌓는다. 논문에서는 총 6개의 인코더 층을 사용하였지만, 실습에서는 학습 시간을 고려하여 그보다 적은 개수를 사용할 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cleared-tradition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-guatemala",
   "metadata": {},
   "source": [
    "## 디코더\n",
    "\n",
    "디코더는 인코더와 비슷하지만, 인코더보다 조금 더 복잡한데 인코더는 두 개의 서브 층으로 구성되지만, 디코더는 세 개의 서브 층으로 구성된다는 점이 다르다.\n",
    "\n",
    "### 디코더 층\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_23_vBHZ3i0.max-800x600.png?raw=true)\n",
    "\n",
    "첫 번째는 셀프 어텐션, 두 번째는 인코더-디코더 어텐션, 세 번째는 피드 포워드 신경망이다. 인코더-디코더 어텐션은 셀프 어텐션과는 달리, Query가 디코더의 벡터인 반면에 Key와 Value가 인코더의 벡터라는 특징이 있다. 이 부분이 인코더가 입력 문장으로부터 정보를 디코더에 전달하는 과정이다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/Untitled_24_Kj9egLY.max-800x600.png?raw=true)\n",
    "인코더의 셀프 어텐션과 마찬가지로 디코더의 셀프 어텐션, 인코더-디코더 어텐션 두 개의 어텐션 모두 스케일드 닷 프로덕트 어텐션을 멀티 헤드 어텐션으로 병렬적으로 수행한다.\n",
    "\n",
    "디코더의 세 개의 서브 층을 내부적으로 구현한 디코더의 함수는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stunning-skiing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-analysis",
   "metadata": {},
   "source": [
    "### 디코더 층을 쌓아 디코더 만들기\n",
    "\n",
    "이렇게 구현한 디코더의 층은 임베딩 층(Embedding layer) 과 포지셔널 인코딩(Positional Encoding) 을 연결하고, 사용자가 원하는 만큼 디코더 층을 쌓아 트랜스포머의 디코더가 완성된다.\n",
    "\n",
    "인코더와 마찬가지로 num_layers 개수의 디코더 층을 쌓는다. 논문에서는 총 6개의 디코더 층을 사용하였지만, 실습에서는 학습 시간을 고려하여 그보다 적은 개수를 사용할 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "varying-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-bishop",
   "metadata": {},
   "source": [
    "## 챗봇의 병렬 데이터 받아오기\n",
    "\n",
    "여기서는 Cornell Movie-Dialogs Corpus라는 영화 및 TV 프로그램에서 사용되었던 대화의 쌍으로 구성된 데이터셋을 사용하는데 대화의 쌍이라고 하는 것은 기본적으로 먼저 말하는 사람의 대화 문장이 있고, 그에 응답하는 대화 문장의 쌍으로 이루어진다.\n",
    "\n",
    "데이터를 받아오는 이번 스텝에서 목표로 하는 것은 다음과 같다.\n",
    "\n",
    "- 정해진 개수인 50,000개의 질문과 답변의 쌍을 추출한다.\n",
    "- 문장에서 단어와 구두점 사이에 공백을 추가한다.\n",
    "- 알파벳과 ! ? , . 이 4개의 구두점을 제외하고 다른 특수문자는 모두 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hundred-university",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'cornell_movie_dialogs.zip',\n",
    "    origin='http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset,'movie_conversations.txt')\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-mechanism",
   "metadata": {},
   "source": [
    "여기서 우리가 사용할 데이터는 실습 시간을 고려하여 전체 데이터 중 일부로, 우선, 데이터 중에서 5만 개만 가져오도록 하고 질문과 답변의 쌍의 형태로 데이터셋을 가공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "distributed-stanford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "# 사용할 샘플의 최대 개수\n",
    "MAX_SAMPLES = 50000\n",
    "print(MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-oklahoma",
   "metadata": {},
   "source": [
    "이를 위한 전처리 함수는 다음과 같다.\n",
    "\n",
    "이번 전처리는 정규 표현식(Regular Expression) 을 사용하여 구두점(punctuation) 을 제거하여 단어를 토크나이징(tokenizing) 하는 일에 방해가 되지 않도록 정제하는 것을 목표로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "commercial-jaguar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "  # student와 온점 사이에 거리를 만듭니다.\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "major-arnold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
    "def load_conversations():\n",
    "  id2line = {}\n",
    "  with open(path_to_movie_lines, errors='ignore') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    id2line[parts[0]] = parts[4]\n",
    "\n",
    "  inputs, outputs = [], []\n",
    "  with open(path_to_movie_conversations, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "\n",
    "    for i in range(len(conversation) - 1):\n",
    "      # 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용.\n",
    "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "\n",
    "      if len(inputs) >= MAX_SAMPLES:\n",
    "        return inputs, outputs\n",
    "  return inputs, outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sharing-light",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 50000\n",
      "전체 샘플 수 : 50000\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
    "questions, answers = load_conversations()\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-champagne",
   "metadata": {},
   "source": [
    "질문과 답변은 병렬적으로 구성되는 데이터셋이므로 두 샘플 수는 정확하게 일치해야 한다.\n",
    "\n",
    "둘 다 5만 개의 샘플이 저장되었다.\n",
    "\n",
    "임의로 22번째 샘플(인덱스 상으로는 21번 샘플)을 출력해서 질문과 답변이 병렬적으로 잘 저장은 되었는지, 그리고 전처리 함수에서 의도했던 전처리가 진행되었는지 확인해 보면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "incorrect-oliver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: she s not a . . .\n",
      "전처리 후의 22번째 답변 샘플: lesbian ? no . i found a picture of jared leto in one of her drawers , so i m pretty sure she s not harboring same sex tendencies .\n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-locator",
   "metadata": {},
   "source": [
    "## 병렬 데이터 전처리하기\n",
    "\n",
    "질문과 답변의 셋을 각각 questions와 answers에 저장하였으므로, 본격적으로 전처리를 진행해보겠습니다. 이번 스텝에서 진행할 전체적인 과정을 요약하면 다음과 같다.\n",
    "\n",
    "- TensorFlow Datasets SubwordTextEncoder를 토크나이저로 사용한다.  단어보다 더 작은 단위인 Subword를 기준으로 토크나이징하고,  각 토큰을 고유한 정수로 인코딩한다.\n",
    "- 각 문장을 토큰화하고 각 문장의 시작과 끝을 나타내는 START_TOKEN 및 END_TOKEN을 추가한다.\n",
    "- 최대 길이 MAX_LENGTH인 40을 넘는 문장들은 필터링한다.\n",
    "- MAX_LENGTH보다 길이가 짧은 문장들은 40에 맞도록 패딩 한다.\n",
    "\n",
    "### 1. 단어장(Vocabulary) 만들기\n",
    "\n",
    "우선 각 단어에 고유한 정수 인덱스를 부여하기 위해서 단어장(Vocabulary)을 만들고, 단어장을 만들 때는 질문과 답변 데이터셋을 모두 사용하여 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "offshore-harvest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n",
      "슝=3 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성. (Tensorflow 2.3.0 이상) (클라우드는 2.4 입니다)\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"슝=3 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-kenya",
   "metadata": {},
   "source": [
    "❗(주의) Tensorflow 2.2.0 이하의 버전에서는 아래 주석의 코드를 대신 실행할것...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "current-trigger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "#tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"슝=3 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-mission",
   "metadata": {},
   "source": [
    "이때 디코더의 문장 생성 과정에서 사용할 '시작 토큰'과 '종료 토큰'에 대해서도 임의로 단어장에 추가하여서 정수를 부여해 준다. 이미 생성된 단어장의 번호와 겹치지 않도록 각각 단어장의 크기와 그보다 1이 큰 수를 번호로 부여한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "packed-dating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sacred-while",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8331]\n",
      "END_TOKEN의 번호 : [8332]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-religious",
   "metadata": {},
   "source": [
    "각각 8,331과 8,332라는 점에서 현재 단어장의 크기가 8,331(0번부터 8,330번)이라는 의미이다.\n",
    "\n",
    "두 개의 토큰을 추가해 주었기 때문에 단어장의 크기도 +2임을 명시해 주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "handed-prayer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8333\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-criminal",
   "metadata": {},
   "source": [
    "### 2. 각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)\n",
    "\n",
    "위에서 tensorflow_datasets의 SubwordTextEncoder를 사용해서 tokenizer를 정의하고 Vocabulary를 만들었다면, tokenizer.encode()로 각 단어를 정수로 변환할 수 있고 또는 tokenizer.decode()를 통해 정수 시퀀스를 단어 시퀀스로 변환할 수 있다.\n",
    "\n",
    "예를 들어서 22번째 샘플을 tokenizer.encode()의 입력으로 사용해서 변환 결과를 보면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "saved-journalist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [60, 8, 37, 8172, 49]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [7824, 1223, 19, 61, 2, 4, 336, 10, 1595, 14, 1104, 698, 3263, 263, 16, 71, 14, 107, 2133, 900, 3, 59, 4, 23, 355, 204, 60, 8, 37, 885, 2289, 8107, 344, 1001, 5179, 4214, 342, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-peripheral",
   "metadata": {},
   "source": [
    "각 단어에 고유한 정수가 부여된 Vocabulary를 기준으로 단어 시퀀스가 정수 시퀀스로 인코딩된 결과를 확인할 수 있다. 위의 결과와 마찬가지로 질문과 답변 셋에 대해서 전부 정수 인코딩을 수행한다. 이와 동시에 문장의 최대 길이를 정하고, 해당 길이로 패딩(padding) 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "rough-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "intended-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "filled-schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8333\n",
      "필터링 후의 질문 샘플 개수: 44095\n",
      "필터링 후의 답변 샘플 개수: 44095\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-optimum",
   "metadata": {},
   "source": [
    "### 3. 교사 강요(Teacher Forcing) 사용하기\n",
    "\n",
    "tf.data.Dataset API는 훈련 프로세스의 속도가 빨라지도록 입력 파이프라인을 구축하는 API이다.\n",
    "\n",
    "이를 적극 사용하기 위해서 질문과 답변의 쌍을 tf.data.Dataset의 입력으로 넣어주는 작업을 한다.\n",
    "\n",
    "이때, 디코더의 입력과 실제값(레이블)을 정의해 주기 위해서는 교사 강요(Teacher Forcing) 이라는 언어 모델의 훈련 기법을 이해해야만 한다. \n",
    "\n",
    "이전 자신의 출력이 현재 자신의 상태를 결정하는 모델을 자기회귀 모델(auto-regressive model, AR) 이라고 한다. \n",
    "\n",
    "RNN 언어 모델은 대표적인 자기 회귀 모델의 예이며, 트랜스포머의 디코더 또한 자기회귀 모델입니다.\n",
    "\n",
    "트랜스포머 디코더에서도 교사 강요(Teacher Forcing) 를 적용.\n",
    "\n",
    "질문과 답변의 쌍을 tf.data.Dataset API의 입력으로 사용하여 파이프라인을 구성하는데 이때, 교사 강요를 위해서 answers[:, :-1]를 디코더의 입력값, answers[:, 1:]를 디코더의 레이블로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "identical-macedonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-newman",
   "metadata": {},
   "source": [
    "## 모델 정의 및 학습하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mechanical-mistake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-italy",
   "metadata": {},
   "source": [
    "### 1. 모델 생성\n",
    "\n",
    "num_layers, d-Model, units는 전부 사용자가 정할 수 있는 하이퍼파라미터 값이다.\n",
    "\n",
    "논문에서 num_layers는 6, d-Model은 512였지만, 빠르고 원활한 훈련을 위해 여기서는 각 하이퍼파라미터를 논문에서보다는 작은 값을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "revolutionary-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3187456     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3714816     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8333)   2141581     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,043,853\n",
      "Trainable params: 9,043,853\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-milton",
   "metadata": {},
   "source": [
    "### 2. 손실 함수(Loss function)\n",
    "\n",
    "레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "comparable-chuck",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-pakistan",
   "metadata": {},
   "source": [
    "### 3. 커스텀 된 학습률(Learning rate)\n",
    "\n",
    "딥러닝 모델학습 시 learning rate는 매우 중요한 하이퍼파라미터다. 최근에는 모델학습 초기에 learning rate를 급격히 높였다가, 이후 train step이 진행됨에 따라 서서히 낮추어 가면서 안정적으로 수렴하게 하는 고급 기법을 널리 사용하고 있다. 이런 방법을 커스텀 학습률 스케줄링(Custom Learning rate Scheduling)이라고 한다.\n",
    "\n",
    "논문에 나온 공식을 참고하여 커스텀 학습률 스케줄러를 통한 아담 옵티마이저를 사용하며, 논문에 나온 공식은 다음과 같다.\n",
    "\n",
    "![](https://github.com/MulderKim/EXPLORATION/blob/main/15/etc/max-800x600-1.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "quick-memphis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-novel",
   "metadata": {},
   "source": [
    "그러면 방금 정의한 커스텀 학습률 스케줄링 계획을 시각화해 보면 위에 언급한 수식은 step_num<sup>-0.5</sup> 에 비례하는 부분과 step_num에 비례하는 부분 중 작은 쪽을 택하도록 되어 있다. 그래서 학습 초기에는 learning_rate가 step_num에 비례해서 증가하다가 이후로는 감소하는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "headed-devon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-marking",
   "metadata": {},
   "source": [
    "### 4. 모델 컴파일\n",
    "\n",
    "손실 함수와 커스텀 된 학습률(learning rate)을 사용하여 모델을 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "hybrid-maple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-joyce",
   "metadata": {},
   "source": [
    "### 5. 훈련하기\n",
    "\n",
    "총 20 에포크를 학습."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "differential-possession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "689/689 [==============================] - 85s 113ms/step - loss: 2.4847 - accuracy: 0.0252\n",
      "Epoch 2/20\n",
      "689/689 [==============================] - 82s 119ms/step - loss: 1.5478 - accuracy: 0.0766\n",
      "Epoch 3/20\n",
      "689/689 [==============================] - 87s 126ms/step - loss: 1.4085 - accuracy: 0.0852\n",
      "Epoch 4/20\n",
      "689/689 [==============================] - 93s 135ms/step - loss: 1.3461 - accuracy: 0.0904\n",
      "Epoch 5/20\n",
      "689/689 [==============================] - 98s 142ms/step - loss: 1.2904 - accuracy: 0.0946\n",
      "Epoch 6/20\n",
      "689/689 [==============================] - 96s 139ms/step - loss: 1.2448 - accuracy: 0.0991\n",
      "Epoch 7/20\n",
      "689/689 [==============================] - 99s 144ms/step - loss: 1.1793 - accuracy: 0.1026\n",
      "Epoch 8/20\n",
      "689/689 [==============================] - 97s 141ms/step - loss: 1.1178 - accuracy: 0.1086\n",
      "Epoch 9/20\n",
      "689/689 [==============================] - 99s 143ms/step - loss: 1.0582 - accuracy: 0.1148\n",
      "Epoch 10/20\n",
      "689/689 [==============================] - 100s 146ms/step - loss: 0.9981 - accuracy: 0.1209\n",
      "Epoch 11/20\n",
      "689/689 [==============================] - 100s 145ms/step - loss: 0.9442 - accuracy: 0.1271\n",
      "Epoch 12/20\n",
      "689/689 [==============================] - 101s 146ms/step - loss: 0.9046 - accuracy: 0.1340\n",
      "Epoch 13/20\n",
      "689/689 [==============================] - 99s 144ms/step - loss: 0.8589 - accuracy: 0.1399\n",
      "Epoch 14/20\n",
      "689/689 [==============================] - 99s 144ms/step - loss: 0.8142 - accuracy: 0.1442\n",
      "Epoch 15/20\n",
      "689/689 [==============================] - 101s 147ms/step - loss: 0.7849 - accuracy: 0.1505\n",
      "Epoch 16/20\n",
      "689/689 [==============================] - 101s 147ms/step - loss: 0.7555 - accuracy: 0.1564\n",
      "Epoch 17/20\n",
      "689/689 [==============================] - 100s 145ms/step - loss: 0.7272 - accuracy: 0.1605\n",
      "Epoch 18/20\n",
      "689/689 [==============================] - 102s 148ms/step - loss: 0.6995 - accuracy: 0.1648\n",
      "Epoch 19/20\n",
      "689/689 [==============================] - 101s 147ms/step - loss: 0.6747 - accuracy: 0.1696\n",
      "Epoch 20/20\n",
      "689/689 [==============================] - 101s 146ms/step - loss: 0.6551 - accuracy: 0.1740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbea052ddd0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-gallery",
   "metadata": {},
   "source": [
    "## 챗봇 테스트하기\n",
    "\n",
    "예측(inference) 단계는 기본적으로 다음과 같은 과정을 거친다.\n",
    "\n",
    "- 새로운 입력 문장에 대해서는 훈련 때와 동일한 전처리를 거친다.\n",
    "- 입력 문장을 토크나이징하고, START_TOKEN과 END_TOKEN을 추가한다.\n",
    "- 패딩 마스킹과 룩 어헤드 마스킹을 계산한다.\n",
    "- 디코더는 입력 시퀀스로부터 다음 단어를 예측한다.\n",
    "- 디코더는 예측된 다음 단어를 기존의 입력 시퀀스에 추가하여 새로운 입력으로 사용한다.\n",
    "- END_TOKEN이 예측되거나 문장의 최대 길이에 도달하면 디코더는 동작을 멈춘다.\n",
    "\n",
    "위의 과정을 모두 담은 decoder_inference() 함수를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "multiple-watch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-visibility",
   "metadata": {},
   "source": [
    "임의의 입력 문장에 대해서 decoder_inference() 함수를 호출하여 챗봇의 대답을 얻는 sentence_generation() 함수를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "minimal-shaft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-employer",
   "metadata": {},
   "source": [
    "임의의 문장으로부터 챗봇의 대답을 얻어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "existing-economy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : Where have you been?\n",
      "출력 : i m a catholic .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i m a catholic .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('Where have you been?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dedicated-labor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : It's a trap\n",
      "출력 : i m sorry , i don t know what to say that .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i m sorry , i don t know what to say that .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"It's a trap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-accuracy",
   "metadata": {},
   "source": [
    "## 프로젝트: 한국어 데이터로 챗봇 만들기\n",
    "\n",
    "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜보자.\n",
    "\n",
    "### Step 1. 데이터 수집하기\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있다.\n",
    "\n",
    "songys/Chatbot_data\n",
    "\n",
    "$ mkdir -p ~/aiffel/EXPLORATION/15/transformer_chatbot/data/\n",
    "\n",
    "$ ln -s ~/data/* ~/aiffel/EXPLORATION/15/transformer_chatbot/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-mechanism",
   "metadata": {},
   "source": [
    "라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "convinced-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-mississippi",
   "metadata": {},
   "source": [
    "데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "seventh-fetish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/aiffel/aiffel/EXPLORATION/15/transformer_chatbot/data/ChatbotData.csv'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file_path = os.getenv('HOME') + '/aiffel/EXPLORATION/15/transformer_chatbot/data/ChatbotData.csv'\n",
    "dataset_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-material",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 전처리하기\n",
    "\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "legitimate-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    # 영어 포함 시 소문자 변환, 양쪽 공백 제거\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # 단어와 구두점 사이 공백 추가\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    # 한글, 알파벳, .,?!을 제외한 문자 공백으로 대체\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "proof-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conversations():\n",
    "    with open(dataset_file_path, errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    inputs, outputs = [], []\n",
    "    with open(dataset_file_path) as file:\n",
    "        lines = csv.reader(file)\n",
    "        next(lines)\n",
    "        for line in lines:\n",
    "            inputs.append(preprocess_sentence(line[0]))\n",
    "            outputs.append(preprocess_sentence(line[1]))\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "sapphire-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n",
      "\n",
      "전처리 후의 11번째 질문 샘플: 가끔 궁금해\n",
      "전처리 후의 11번째 답변 샘플: 그 사람도 그럴 거예요 .\n"
     ]
    }
   ],
   "source": [
    "questions, answers = load_conversations()\n",
    "\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))\n",
    "print()\n",
    "print('전처리 후의 11번째 질문 샘플: {}'.format(questions[11]))\n",
    "print('전처리 후의 11번째 답변 샘플: {}'.format(answers[11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-willow",
   "metadata": {},
   "source": [
    "### Step 3. SubwordTextEncoder 사용하기\n",
    "\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-taste",
   "metadata": {},
   "source": [
    "토크나이징\n",
    "단어장(Vocabulary) 만들기 - SubwordTextEncoder 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "atomic-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "hungarian-pakistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8133]\n",
      "END_TOKEN의 번호 : [8134]\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수 부여\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "regulation-silly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8135\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-austin",
   "metadata": {},
   "source": [
    "정수 인코딩 & 패딩 & start, end token 추가\n",
    "- tokenizer.encode() : 단어 > 정수\n",
    "- tokenizer.decode() : 정수 > 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "instructional-grass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions 최대 길이 : 16\n",
      "questions 평균 길이 : 3.9391017508246637\n",
      "answers 최대 길이 : 24\n",
      "answers 평균 길이 : 4.716146494121627\n"
     ]
    }
   ],
   "source": [
    "questions_len = [len(s.split()) for s in questions]\n",
    "answers_len = [len(s.split()) for s in answers]\n",
    "print(f'questions 최대 길이 : {np.max(questions_len)}')\n",
    "print(f'questions 평균 길이 : {np.mean(questions_len)}')\n",
    "print(f'answers 최대 길이 : {np.max(answers_len)}')\n",
    "print(f'answers 평균 길이 : {np.mean(answers_len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "painful-feeling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('questions')\n",
    "plt.hist(questions_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('answers')\n",
    "plt.hist(answers_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "specified-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 허용 길이 지정\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# 정수인코딩, 최대길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # 정수 인코딩 과정에서 시작, 종료 토큰 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        \n",
    "        # 최대 길이 10 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "    \n",
    "    # 최대 길이 10으로 모든 데이터셋 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post'\n",
    "    )\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post'\n",
    "    )\n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "excellent-banana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8135\n",
      "필터링 후의 질문 샘플 개수: 9128\n",
      "필터링 후의 답변 샘플 개수: 9128\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-preserve",
   "metadata": {},
   "source": [
    "교사 강요(Teacher Forcing) 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "printable-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "smart-sydney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ({inputs: (None, 10), dec_inputs: (None, 9)}, {outputs: (None, 9)}), types: ({inputs: tf.int32, dec_inputs: tf.int32}, {outputs: tf.int32})>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-convenience",
   "metadata": {},
   "source": [
    "### Step 4. 모델 구성하기\n",
    "\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "swiss-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model\n",
    "        )\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-lobby",
   "metadata": {},
   "source": [
    "Scaled Dot Product Attention\n",
    "\n",
    "내적(dot product)을 통해 단어 벡터 간 유사도를 구한 후에, 특정 값을 분모로 나눠주는 방식으로 Q와 K의 유사도를 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "above-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"어텐션 가중치를 계산. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)  # # tf.matmul : 두 텐서를 행렬곱한 결과 텐서를 리턴\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-fusion",
   "metadata": {},
   "source": [
    "Multi Head Attention\n",
    "- 내부적으로는 스케일드 닷 프로덕트 어테션 함수를 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "southwest-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다.\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다.\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-suggestion",
   "metadata": {},
   "source": [
    "Masking\n",
    "\n",
    "특정 값들을 가려서 실제 연산에 방해가 되지 않도록 하는 기법\n",
    "\n",
    "Padding Masking\n",
    "\n",
    "문자의 길이를 맞추기 위해서 패딩을 넣어 처리해준 부분은 실제 의미가 있는 단어가 아니므로 패딩부분을 마스킹하여 가려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "actual-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0인 위치에서 숫자 1이 나오고, 숫자0이 아닌 위치는 숫자0이 나오게 하는 함수\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-landscape",
   "metadata": {},
   "source": [
    "Look-ahead Masking 다음 단어 가리기\n",
    "\n",
    "RNN은 각 step마다 순서대로 들어가는 반면, 트랜스포머는 문장행렬을 만들어 한 번에 행렬형태로 입력으로 들어가기 때문에 마스킹을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "hybrid-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-learning",
   "metadata": {},
   "source": [
    "인코더\n",
    "\n",
    "- 하나의 인코더 층은 2개의 서브 층으로 나뉜다.\n",
    "  - 셀프어텐션(멀티 헤드 어텐션)\n",
    "  - 피드 포워드 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "driving-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "    })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "hollow-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-capacity",
   "metadata": {},
   "source": [
    "디코더\n",
    "- 셀프 어텐션\n",
    "- 인코더-디코더 어텐션 : query가 디코더의 벡터, key-value가 인코더의 벡터\n",
    "- 피드 포워드 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "driving-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "        'query': inputs,\n",
    "        'key': inputs,\n",
    "        'value': inputs,\n",
    "        'mask': look_ahead_mask\n",
    "    })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "        'query': attention1,\n",
    "        'key': enc_outputs,\n",
    "        'value': enc_outputs,\n",
    "        'mask': padding_mask\n",
    "    })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "desperate-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 층을 쌓아 디코더 만들기\n",
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-audit",
   "metadata": {},
   "source": [
    "모델 정의 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "therapeutic-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask,\n",
    "        output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask'\n",
    "    )(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask'\n",
    "    )(dec_inputs)\n",
    "\n",
    "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask,\n",
    "        output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask'\n",
    "    )(inputs)\n",
    "\n",
    "    # 인코더\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-nigeria",
   "metadata": {},
   "source": [
    "모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "demographic-cornell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3136768     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3664128     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8135)   2090695     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,891,591\n",
      "Trainable params: 8,891,591\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256  # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8  # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512    # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1  # 드롭아웃 비율\n",
    "\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-dream",
   "metadata": {},
   "source": [
    "손실함수\n",
    "\n",
    "레이블인 시퀀스에 패딩이 되어져 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "earned-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-intranet",
   "metadata": {},
   "source": [
    "커스텀된 학습률 learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "specialized-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "impressive-vector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-klein",
   "metadata": {},
   "source": [
    "모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "needed-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-bankruptcy",
   "metadata": {},
   "source": [
    "훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "surprised-outline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "143/143 [==============================] - 14s 51ms/step - loss: 5.8032 - accuracy: 0.0584\n",
      "Epoch 2/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 4.9646 - accuracy: 0.2031\n",
      "Epoch 3/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 4.0789 - accuracy: 0.2149\n",
      "Epoch 4/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 3.5475 - accuracy: 0.2172\n",
      "Epoch 5/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 3.3414 - accuracy: 0.2280\n",
      "Epoch 6/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 3.1020 - accuracy: 0.2403\n",
      "Epoch 7/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 2.9269 - accuracy: 0.2538\n",
      "Epoch 8/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 2.7049 - accuracy: 0.2767\n",
      "Epoch 9/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 2.4685 - accuracy: 0.3045\n",
      "Epoch 10/50\n",
      "143/143 [==============================] - 7s 49ms/step - loss: 2.2083 - accuracy: 0.3364\n",
      "Epoch 11/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 1.9230 - accuracy: 0.3727\n",
      "Epoch 12/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 1.6456 - accuracy: 0.4084\n",
      "Epoch 13/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 1.3502 - accuracy: 0.4481\n",
      "Epoch 14/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 1.1023 - accuracy: 0.4867\n",
      "Epoch 15/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.8417 - accuracy: 0.5255\n",
      "Epoch 16/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.6318 - accuracy: 0.5634\n",
      "Epoch 17/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.4536 - accuracy: 0.5873\n",
      "Epoch 18/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.3169 - accuracy: 0.6150\n",
      "Epoch 19/50\n",
      "143/143 [==============================] - 7s 49ms/step - loss: 0.2241 - accuracy: 0.6295\n",
      "Epoch 20/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.1584 - accuracy: 0.6374\n",
      "Epoch 21/50\n",
      "143/143 [==============================] - 7s 51ms/step - loss: 0.1189 - accuracy: 0.6436\n",
      "Epoch 22/50\n",
      "143/143 [==============================] - 7s 51ms/step - loss: 0.1046 - accuracy: 0.6468\n",
      "Epoch 23/50\n",
      "143/143 [==============================] - 7s 51ms/step - loss: 0.0883 - accuracy: 0.6463\n",
      "Epoch 24/50\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0856 - accuracy: 0.6493\n",
      "Epoch 25/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.0816 - accuracy: 0.6506\n",
      "Epoch 26/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.0752 - accuracy: 0.6496\n",
      "Epoch 27/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.0789 - accuracy: 0.6469\n",
      "Epoch 28/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.0754 - accuracy: 0.6449\n",
      "Epoch 29/50\n",
      "143/143 [==============================] - 7s 51ms/step - loss: 0.0760 - accuracy: 0.6494\n",
      "Epoch 30/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.0659 - accuracy: 0.6497\n",
      "Epoch 31/50\n",
      "143/143 [==============================] - 7s 51ms/step - loss: 0.0602 - accuracy: 0.6523\n",
      "Epoch 32/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.0557 - accuracy: 0.6516\n",
      "Epoch 33/50\n",
      "143/143 [==============================] - 7s 49ms/step - loss: 0.0478 - accuracy: 0.6562\n",
      "Epoch 34/50\n",
      "143/143 [==============================] - 7s 49ms/step - loss: 0.0212 - accuracy: 0.6613\n",
      "Epoch 50/50\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.0182 - accuracy: 0.6621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbeec7b0b10>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-alarm",
   "metadata": {},
   "source": [
    "### Step 5. 모델 평가하기\n",
    "\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "focused-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "    # ex) Where have you been? → [[8331  86  30  5  1059  7  8332]]\n",
    "    sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "specialized-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 입력 문장에 대해서 decoder_inference() 함수를 호출하여 챗봇의 대답을 얻는 함수\n",
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "architectural-clearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 피곤해\n",
      "출력 : 좀 더 일찍 잠자리에 들어보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'좀 더 일찍 잠자리에 들어보세요 .'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('피곤해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "spread-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 졸려 죽겠어\n",
      "출력 : 제가 곁에 있을게요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'제가 곁에 있을게요 .'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('졸려 죽겠어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "colonial-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 주말이 언제 올까?\n",
      "출력 : 서로를 알게 되었겠네요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'서로를 알게 되었겠네요 .'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('주말이 언제 올까?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "macro-illustration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 뭐먹지?\n",
      "출력 : 어이 없는 일이 있었나봐요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'어이 없는 일이 있었나봐요 .'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘 뭐먹지?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-martial",
   "metadata": {},
   "source": [
    "D_MODEL = 256 -> 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "crude-secondary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 512)    7321088     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    9424384     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8135)   4173255     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 20,918,727\n",
      "Trainable params: 20,918,727\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 512  # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8  # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512    # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1  # 드롭아웃 비율\n",
    "\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "prime-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "nonprofit-blind",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "143/143 [==============================] - 20s 91ms/step - loss: 5.6662 - accuracy: 0.0584\n",
      "Epoch 2/50\n",
      "143/143 [==============================] - 13s 94ms/step - loss: 4.2517 - accuracy: 0.2131\n",
      "Epoch 3/50\n",
      "143/143 [==============================] - 13s 92ms/step - loss: 3.6428 - accuracy: 0.2170\n",
      "Epoch 4/50\n",
      "143/143 [==============================] - 14s 97ms/step - loss: 3.3865 - accuracy: 0.2278\n",
      "Epoch 5/50\n",
      "143/143 [==============================] - 13s 94ms/step - loss: 3.1755 - accuracy: 0.2385\n",
      "Epoch 6/50\n",
      "143/143 [==============================] - 14s 95ms/step - loss: 2.9193 - accuracy: 0.2540\n",
      "Epoch 7/50\n",
      "143/143 [==============================] - 14s 95ms/step - loss: 2.6633 - accuracy: 0.2816\n",
      "Epoch 8/50\n",
      "143/143 [==============================] - 14s 98ms/step - loss: 2.3786 - accuracy: 0.3198\n",
      "Epoch 9/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 2.0750 - accuracy: 0.3580\n",
      "Epoch 10/50\n",
      "143/143 [==============================] - 14s 97ms/step - loss: 1.7285 - accuracy: 0.4010\n",
      "Epoch 11/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 1.4048 - accuracy: 0.4493\n",
      "Epoch 12/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 1.0779 - accuracy: 0.4969\n",
      "Epoch 13/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.8038 - accuracy: 0.5425\n",
      "Epoch 14/50\n",
      "143/143 [==============================] - 14s 98ms/step - loss: 0.5561 - accuracy: 0.5835\n",
      "Epoch 15/50\n",
      "143/143 [==============================] - 14s 98ms/step - loss: 0.3658 - accuracy: 0.6140\n",
      "Epoch 16/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.2350 - accuracy: 0.6394\n",
      "Epoch 17/50\n",
      "143/143 [==============================] - 14s 97ms/step - loss: 0.1489 - accuracy: 0.6461\n",
      "Epoch 18/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.1093 - accuracy: 0.6521\n",
      "Epoch 19/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 0.0899 - accuracy: 0.6507\n",
      "Epoch 20/50\n",
      "143/143 [==============================] - 15s 101ms/step - loss: 0.0815 - accuracy: 0.6501\n",
      "Epoch 21/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 0.0690 - accuracy: 0.6528\n",
      "Epoch 22/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.0702 - accuracy: 0.6508\n",
      "Epoch 23/50\n",
      "143/143 [==============================] - 15s 104ms/step - loss: 0.0719 - accuracy: 0.6518\n",
      "Epoch 24/50\n",
      "143/143 [==============================] - 14s 101ms/step - loss: 0.0681 - accuracy: 0.6512\n",
      "Epoch 25/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 0.0671 - accuracy: 0.6537\n",
      "Epoch 26/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 0.0700 - accuracy: 0.6473\n",
      "Epoch 27/50\n",
      "143/143 [==============================] - 14s 101ms/step - loss: 0.0698 - accuracy: 0.6487\n",
      "Epoch 28/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 0.0695 - accuracy: 0.6493\n",
      "Epoch 29/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 0.0737 - accuracy: 0.6489\n",
      "Epoch 30/50\n",
      "143/143 [==============================] - 14s 101ms/step - loss: 0.0691 - accuracy: 0.6497\n",
      "Epoch 31/50\n",
      "143/143 [==============================] - 14s 97ms/step - loss: 0.0605 - accuracy: 0.6543\n",
      "Epoch 32/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.0559 - accuracy: 0.6546\n",
      "Epoch 33/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 0.0512 - accuracy: 0.6557\n",
      "Epoch 34/50\n",
      "143/143 [==============================] - 14s 101ms/step - loss: 0.0499 - accuracy: 0.6578\n",
      "Epoch 35/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.0456 - accuracy: 0.6560\n",
      "Epoch 36/50\n",
      "143/143 [==============================] - 14s 101ms/step - loss: 0.0450 - accuracy: 0.6548\n",
      "Epoch 37/50\n",
      "143/143 [==============================] - 14s 101ms/step - loss: 0.0427 - accuracy: 0.6577\n",
      "Epoch 38/50\n",
      "143/143 [==============================] - 14s 98ms/step - loss: 0.0377 - accuracy: 0.6561\n",
      "Epoch 39/50\n",
      "143/143 [==============================] - 14s 101ms/step - loss: 0.0405 - accuracy: 0.6563\n",
      "Epoch 40/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 0.0335 - accuracy: 0.6581\n",
      "Epoch 41/50\n",
      "143/143 [==============================] - 14s 100ms/step - loss: 0.0323 - accuracy: 0.6584\n",
      "Epoch 42/50\n",
      "143/143 [==============================] - 14s 101ms/step - loss: 0.0324 - accuracy: 0.6560\n",
      "Epoch 43/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.0295 - accuracy: 0.6626\n",
      "Epoch 44/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.0281 - accuracy: 0.6581\n",
      "Epoch 45/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.0288 - accuracy: 0.6610\n",
      "Epoch 46/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.0271 - accuracy: 0.6584\n",
      "Epoch 47/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.0261 - accuracy: 0.6604\n",
      "Epoch 48/50\n",
      "143/143 [==============================] - 14s 98ms/step - loss: 0.0286 - accuracy: 0.6608\n",
      "Epoch 49/50\n",
      "143/143 [==============================] - 15s 102ms/step - loss: 0.0215 - accuracy: 0.6629\n",
      "Epoch 50/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.0201 - accuracy: 0.6635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbe715b9a10>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "least-tulsa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 피곤해\n",
      "출력 : 충전하는 시간 그 자체로 소중합니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'충전하는 시간 그 자체로 소중합니다 .'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('피곤해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "individual-practice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 졸려 죽겠어\n",
      "출력 : 바람이라도 쐬고 오는 건 어떨까요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'바람이라도 쐬고 오는 건 어떨까요 .'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('졸려 죽겠어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "diverse-prospect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 주말이 언제 올까?\n",
      "출력 : 그럼요 . 연락해보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그럼요 . 연락해보세요 .'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('주말이 언제 올까?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "boolean-quick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 뭐먹지?\n",
      "출력 : 맛있는 드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'맛있는 드세요 .'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘 뭐먹지?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-short",
   "metadata": {},
   "source": [
    "NUM_LAYERS = 2 -> 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "moderate-undergraduate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 512)    12055040    inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    17313280    dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8135)   4173255     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 33,541,575\n",
      "Trainable params: 33,541,575\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 5 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 512  # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8  # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512    # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1  # 드롭아웃 비율\n",
    "\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "established-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "unknown-growing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "143/143 [==============================] - 39s 164ms/step - loss: 5.5960 - accuracy: 0.0620\n",
      "Epoch 2/50\n",
      "143/143 [==============================] - 24s 167ms/step - loss: 4.3026 - accuracy: 0.2000\n",
      "Epoch 3/50\n",
      "143/143 [==============================] - 24s 171ms/step - loss: 3.6645 - accuracy: 0.2165\n",
      "Epoch 4/50\n",
      "143/143 [==============================] - 25s 172ms/step - loss: 3.4603 - accuracy: 0.2210\n",
      "Epoch 5/50\n",
      "143/143 [==============================] - 25s 176ms/step - loss: 3.2953 - accuracy: 0.2298\n",
      "Epoch 6/50\n",
      "143/143 [==============================] - 25s 175ms/step - loss: 3.1325 - accuracy: 0.2401\n",
      "Epoch 7/50\n",
      "143/143 [==============================] - 25s 175ms/step - loss: 2.9493 - accuracy: 0.2542\n",
      "Epoch 8/50\n",
      "143/143 [==============================] - 26s 180ms/step - loss: 2.7234 - accuracy: 0.2725\n",
      "Epoch 9/50\n",
      "143/143 [==============================] - 26s 179ms/step - loss: 2.4601 - accuracy: 0.3005\n",
      "Epoch 10/50\n",
      "143/143 [==============================] - 26s 178ms/step - loss: 2.2032 - accuracy: 0.3291\n",
      "Epoch 11/50\n",
      "143/143 [==============================] - 26s 180ms/step - loss: 1.9366 - accuracy: 0.3645\n",
      "Epoch 12/50\n",
      "143/143 [==============================] - 26s 181ms/step - loss: 1.6435 - accuracy: 0.4023\n",
      "Epoch 13/50\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 1.3995 - accuracy: 0.4408\n",
      "Epoch 14/50\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 1.1349 - accuracy: 0.4736\n",
      "Epoch 15/50\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.9337 - accuracy: 0.5066\n",
      "Epoch 16/50\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.7299 - accuracy: 0.5316\n",
      "Epoch 17/50\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.5767 - accuracy: 0.5563\n",
      "Epoch 18/50\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.4591 - accuracy: 0.5817\n",
      "Epoch 19/50\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.3712 - accuracy: 0.5926\n",
      "Epoch 20/50\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.3056 - accuracy: 0.6034\n",
      "Epoch 21/50\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.2706 - accuracy: 0.6127\n",
      "Epoch 22/50\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.2377 - accuracy: 0.6167\n",
      "Epoch 23/50\n",
      "143/143 [==============================] - 27s 189ms/step - loss: 0.2155 - accuracy: 0.6180\n",
      "Epoch 24/50\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.2076 - accuracy: 0.6213\n",
      "Epoch 25/50\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.2011 - accuracy: 0.6189\n",
      "Epoch 26/50\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.1933 - accuracy: 0.6175\n",
      "Epoch 27/50\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.1885 - accuracy: 0.6198\n",
      "Epoch 28/50\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.1949 - accuracy: 0.6187\n",
      "Epoch 29/50\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.1839 - accuracy: 0.6221\n",
      "Epoch 30/50\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.1730 - accuracy: 0.6232\n",
      "Epoch 31/50\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.1581 - accuracy: 0.6246\n",
      "Epoch 32/50\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.1413 - accuracy: 0.6288\n",
      "Epoch 33/50\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.1347 - accuracy: 0.6329\n",
      "Epoch 34/50\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.1198 - accuracy: 0.6344\n",
      "Epoch 35/50\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.1126 - accuracy: 0.6376\n",
      "Epoch 36/50\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.1084 - accuracy: 0.6370\n",
      "Epoch 37/50\n",
      "143/143 [==============================] - 26s 181ms/step - loss: 0.1021 - accuracy: 0.6417\n",
      "Epoch 38/50\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0938 - accuracy: 0.6423\n",
      "Epoch 39/50\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0813 - accuracy: 0.6448\n",
      "Epoch 40/50\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0877 - accuracy: 0.6454\n",
      "Epoch 41/50\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0740 - accuracy: 0.6500\n",
      "Epoch 42/50\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0766 - accuracy: 0.6486\n",
      "Epoch 43/50\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0637 - accuracy: 0.6519\n",
      "Epoch 44/50\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0561 - accuracy: 0.6528\n",
      "Epoch 48/50\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0532 - accuracy: 0.6544\n",
      "Epoch 49/50\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0535 - accuracy: 0.6527\n",
      "Epoch 50/50\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0487 - accuracy: 0.6530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbe5ce2c150>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "thermal-plaintiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 피곤해\n",
      "출력 : 푹 쉬세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'푹 쉬세요 .'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('피곤해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "annual-arizona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 졸려 죽겠어\n",
      "출력 : 약 먹고 쉬어야 해요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'약 먹고 쉬어야 해요 .'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('졸려 죽겠어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "spanish-warrant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 주말이 언제 올까?\n",
      "출력 : 오래 볼 수 있어 좋은 가봐요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'오래 볼 수 있어 좋은 가봐요 .'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('주말이 언제 올까?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "north-pathology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 뭐먹지?\n",
      "출력 : 색다른걸 드셔보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'색다른걸 드셔보세요 .'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘 뭐먹지?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-function",
   "metadata": {},
   "source": [
    "EPOCHS = 50 ->200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "current-acceptance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 512)    12055040    inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    17313280    dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8135)   4173255     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 33,541,575\n",
      "Trainable params: 33,541,575\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 5 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 512  # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8  # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512    # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1  # 드롭아웃 비율\n",
    "\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "interim-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "extreme-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "143/143 [==============================] - 39s 161ms/step - loss: 5.6611 - accuracy: 0.0559\n",
      "Epoch 2/200\n",
      "143/143 [==============================] - 24s 165ms/step - loss: 4.3517 - accuracy: 0.1761\n",
      "Epoch 3/200\n",
      "143/143 [==============================] - 24s 171ms/step - loss: 3.6460 - accuracy: 0.2157\n",
      "Epoch 4/200\n",
      "143/143 [==============================] - 25s 175ms/step - loss: 3.4396 - accuracy: 0.2226\n",
      "Epoch 5/200\n",
      "143/143 [==============================] - 26s 180ms/step - loss: 3.2843 - accuracy: 0.2292\n",
      "Epoch 6/200\n",
      "143/143 [==============================] - 26s 181ms/step - loss: 3.1604 - accuracy: 0.2400\n",
      "Epoch 7/200\n",
      "143/143 [==============================] - 26s 181ms/step - loss: 2.9572 - accuracy: 0.2535\n",
      "Epoch 8/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 2.7158 - accuracy: 0.2721\n",
      "Epoch 9/200\n",
      "143/143 [==============================] - 26s 179ms/step - loss: 2.4751 - accuracy: 0.2995\n",
      "Epoch 10/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 2.2021 - accuracy: 0.3327\n",
      "Epoch 11/200\n",
      "143/143 [==============================] - 26s 182ms/step - loss: 1.9061 - accuracy: 0.3682\n",
      "Epoch 12/200\n",
      "143/143 [==============================] - 26s 183ms/step - loss: 1.6255 - accuracy: 0.4048\n",
      "Epoch 13/200\n",
      "143/143 [==============================] - 26s 182ms/step - loss: 1.3604 - accuracy: 0.4449\n",
      "Epoch 14/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 1.0996 - accuracy: 0.4781\n",
      "Epoch 15/200\n",
      "143/143 [==============================] - 27s 185ms/step - loss: 0.8870 - accuracy: 0.5095\n",
      "Epoch 16/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.7096 - accuracy: 0.5368\n",
      "Epoch 17/200\n",
      "143/143 [==============================] - 26s 183ms/step - loss: 0.5552 - accuracy: 0.5672\n",
      "Epoch 18/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.4480 - accuracy: 0.5831\n",
      "Epoch 19/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.3536 - accuracy: 0.5985\n",
      "Epoch 20/200\n",
      "143/143 [==============================] - 26s 183ms/step - loss: 0.2957 - accuracy: 0.6039\n",
      "Epoch 21/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.2573 - accuracy: 0.6118\n",
      "Epoch 22/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.2389 - accuracy: 0.6155\n",
      "Epoch 23/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.2203 - accuracy: 0.6186\n",
      "Epoch 24/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.2009 - accuracy: 0.6209\n",
      "Epoch 25/200\n",
      "143/143 [==============================] - 27s 189ms/step - loss: 0.1983 - accuracy: 0.6220\n",
      "Epoch 26/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.2050 - accuracy: 0.6136\n",
      "Epoch 27/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.1944 - accuracy: 0.6192\n",
      "Epoch 28/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.1907 - accuracy: 0.6193\n",
      "Epoch 29/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.1918 - accuracy: 0.6207\n",
      "Epoch 30/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.1719 - accuracy: 0.6218\n",
      "Epoch 31/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.1621 - accuracy: 0.6267\n",
      "Epoch 32/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.1435 - accuracy: 0.6314\n",
      "Epoch 33/200\n",
      "143/143 [==============================] - 26s 182ms/step - loss: 0.1266 - accuracy: 0.6354\n",
      "Epoch 34/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.1237 - accuracy: 0.6358\n",
      "Epoch 35/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.1117 - accuracy: 0.6390\n",
      "Epoch 36/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.1059 - accuracy: 0.6361\n",
      "Epoch 37/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.1020 - accuracy: 0.6392\n",
      "Epoch 38/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0937 - accuracy: 0.6422\n",
      "Epoch 39/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0828 - accuracy: 0.6465\n",
      "Epoch 40/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0800 - accuracy: 0.6467\n",
      "Epoch 41/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0736 - accuracy: 0.6494\n",
      "Epoch 42/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0672 - accuracy: 0.6545\n",
      "Epoch 43/200\n",
      "143/143 [==============================] - 27s 185ms/step - loss: 0.0759 - accuracy: 0.6468\n",
      "Epoch 44/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0659 - accuracy: 0.6502\n",
      "Epoch 45/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0570 - accuracy: 0.6540\n",
      "Epoch 46/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0597 - accuracy: 0.6511\n",
      "Epoch 47/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0576 - accuracy: 0.6502\n",
      "Epoch 48/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0545 - accuracy: 0.6516\n",
      "Epoch 49/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0508 - accuracy: 0.6551\n",
      "Epoch 50/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0510 - accuracy: 0.6552\n",
      "Epoch 51/200\n",
      "143/143 [==============================] - 27s 190ms/step - loss: 0.0447 - accuracy: 0.6537\n",
      "Epoch 52/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0453 - accuracy: 0.6543\n",
      "Epoch 53/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0423 - accuracy: 0.6577\n",
      "Epoch 54/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0419 - accuracy: 0.6533\n",
      "Epoch 55/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0378 - accuracy: 0.6572\n",
      "Epoch 56/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0368 - accuracy: 0.6588\n",
      "Epoch 57/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0380 - accuracy: 0.6601\n",
      "Epoch 58/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0368 - accuracy: 0.6587\n",
      "Epoch 59/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0333 - accuracy: 0.6561\n",
      "Epoch 60/200\n",
      "143/143 [==============================] - 27s 189ms/step - loss: 0.0340 - accuracy: 0.6561\n",
      "Epoch 61/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0325 - accuracy: 0.6579\n",
      "Epoch 62/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0307 - accuracy: 0.6577\n",
      "Epoch 63/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0337 - accuracy: 0.6554\n",
      "Epoch 64/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0297 - accuracy: 0.6577\n",
      "Epoch 65/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0272 - accuracy: 0.6639\n",
      "Epoch 66/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0267 - accuracy: 0.6570\n",
      "Epoch 67/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0296 - accuracy: 0.6622\n",
      "Epoch 68/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0278 - accuracy: 0.6600\n",
      "Epoch 69/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0261 - accuracy: 0.6593\n",
      "Epoch 70/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0233 - accuracy: 0.6627\n",
      "Epoch 71/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0239 - accuracy: 0.6633\n",
      "Epoch 72/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0240 - accuracy: 0.6593\n",
      "Epoch 73/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0231 - accuracy: 0.6623\n",
      "Epoch 74/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0223 - accuracy: 0.6601\n",
      "Epoch 75/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0209 - accuracy: 0.6601\n",
      "Epoch 76/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0210 - accuracy: 0.6604\n",
      "Epoch 77/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0197 - accuracy: 0.6648\n",
      "Epoch 78/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0212 - accuracy: 0.6634\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0222 - accuracy: 0.6631\n",
      "Epoch 80/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0195 - accuracy: 0.6616\n",
      "Epoch 81/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0193 - accuracy: 0.6631\n",
      "Epoch 82/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0191 - accuracy: 0.6603\n",
      "Epoch 83/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0196 - accuracy: 0.6649\n",
      "Epoch 84/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0155 - accuracy: 0.6649\n",
      "Epoch 85/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0204 - accuracy: 0.6605\n",
      "Epoch 86/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0209 - accuracy: 0.6587\n",
      "Epoch 87/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0165 - accuracy: 0.6621\n",
      "Epoch 88/200\n",
      "143/143 [==============================] - 26s 183ms/step - loss: 0.0162 - accuracy: 0.6648\n",
      "Epoch 89/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0163 - accuracy: 0.6635\n",
      "Epoch 90/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0169 - accuracy: 0.6627\n",
      "Epoch 91/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0155 - accuracy: 0.6645\n",
      "Epoch 92/200\n",
      "143/143 [==============================] - 27s 190ms/step - loss: 0.0160 - accuracy: 0.6598\n",
      "Epoch 93/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0147 - accuracy: 0.6624\n",
      "Epoch 94/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0155 - accuracy: 0.6650\n",
      "Epoch 95/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0161 - accuracy: 0.6612\n",
      "Epoch 96/200\n",
      "143/143 [==============================] - 27s 189ms/step - loss: 0.0146 - accuracy: 0.6623\n",
      "Epoch 97/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0168 - accuracy: 0.6651\n",
      "Epoch 98/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0164 - accuracy: 0.6636\n",
      "Epoch 99/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0152 - accuracy: 0.6637\n",
      "Epoch 100/200\n",
      "143/143 [==============================] - 27s 189ms/step - loss: 0.0155 - accuracy: 0.6623\n",
      "Epoch 101/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0142 - accuracy: 0.6638\n",
      "Epoch 102/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0137 - accuracy: 0.6636\n",
      "Epoch 103/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0147 - accuracy: 0.6643\n",
      "Epoch 104/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0126 - accuracy: 0.6642\n",
      "Epoch 105/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0137 - accuracy: 0.6647\n",
      "Epoch 106/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0126 - accuracy: 0.6653\n",
      "Epoch 107/200\n",
      "143/143 [==============================] - 27s 185ms/step - loss: 0.0142 - accuracy: 0.6638\n",
      "Epoch 108/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0132 - accuracy: 0.6636\n",
      "Epoch 109/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0135 - accuracy: 0.6607\n",
      "Epoch 110/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0113 - accuracy: 0.6639\n",
      "Epoch 111/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0142 - accuracy: 0.6638\n",
      "Epoch 112/200\n",
      "143/143 [==============================] - 27s 185ms/step - loss: 0.0114 - accuracy: 0.6635\n",
      "Epoch 113/200\n",
      "143/143 [==============================] - 27s 189ms/step - loss: 0.0125 - accuracy: 0.6646\n",
      "Epoch 114/200\n",
      "143/143 [==============================] - 26s 183ms/step - loss: 0.0117 - accuracy: 0.6598\n",
      "Epoch 115/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0113 - accuracy: 0.6611\n",
      "Epoch 116/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0118 - accuracy: 0.6618\n",
      "Epoch 117/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0123 - accuracy: 0.6648\n",
      "Epoch 118/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0106 - accuracy: 0.6666\n",
      "Epoch 119/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0107 - accuracy: 0.6671\n",
      "Epoch 120/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0105 - accuracy: 0.6637\n",
      "Epoch 121/200\n",
      "143/143 [==============================] - 26s 183ms/step - loss: 0.0130 - accuracy: 0.6637\n",
      "Epoch 122/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0103 - accuracy: 0.6628\n",
      "Epoch 123/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0106 - accuracy: 0.6646\n",
      "Epoch 124/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0110 - accuracy: 0.6599\n",
      "Epoch 125/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0130 - accuracy: 0.6613\n",
      "Epoch 126/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0095 - accuracy: 0.6638\n",
      "Epoch 127/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0101 - accuracy: 0.6652\n",
      "Epoch 128/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0126 - accuracy: 0.6637\n",
      "Epoch 129/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0103 - accuracy: 0.6603\n",
      "Epoch 130/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0108 - accuracy: 0.6636\n",
      "Epoch 131/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0113 - accuracy: 0.6649\n",
      "Epoch 132/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0100 - accuracy: 0.6629\n",
      "Epoch 133/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0104 - accuracy: 0.6637\n",
      "Epoch 134/200\n",
      "143/143 [==============================] - 27s 185ms/step - loss: 0.0103 - accuracy: 0.6646\n",
      "Epoch 135/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0086 - accuracy: 0.6657\n",
      "Epoch 136/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0091 - accuracy: 0.6646\n",
      "Epoch 137/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0099 - accuracy: 0.6620\n",
      "Epoch 138/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0090 - accuracy: 0.6628\n",
      "Epoch 139/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0131 - accuracy: 0.6640\n",
      "Epoch 140/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0106 - accuracy: 0.6642\n",
      "Epoch 141/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0099 - accuracy: 0.6626\n",
      "Epoch 142/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0096 - accuracy: 0.6631\n",
      "Epoch 143/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0090 - accuracy: 0.6628\n",
      "Epoch 144/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0098 - accuracy: 0.6623\n",
      "Epoch 145/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0074 - accuracy: 0.6667\n",
      "Epoch 146/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0092 - accuracy: 0.6631\n",
      "Epoch 147/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0093 - accuracy: 0.6617\n",
      "Epoch 148/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0138 - accuracy: 0.6635\n",
      "Epoch 149/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0076 - accuracy: 0.6645\n",
      "Epoch 150/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0076 - accuracy: 0.6610\n",
      "Epoch 151/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0093 - accuracy: 0.6644\n",
      "Epoch 156/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0077 - accuracy: 0.6619\n",
      "Epoch 157/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0093 - accuracy: 0.6639\n",
      "Epoch 158/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0117 - accuracy: 0.6645\n",
      "Epoch 159/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0074 - accuracy: 0.6635\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 27s 189ms/step - loss: 0.0070 - accuracy: 0.6650\n",
      "Epoch 161/200\n",
      "143/143 [==============================] - 27s 191ms/step - loss: 0.0081 - accuracy: 0.6647\n",
      "Epoch 162/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0085 - accuracy: 0.6639\n",
      "Epoch 163/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0078 - accuracy: 0.6606\n",
      "Epoch 164/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0083 - accuracy: 0.6633\n",
      "Epoch 165/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0105 - accuracy: 0.6612\n",
      "Epoch 166/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0058 - accuracy: 0.6619\n",
      "Epoch 167/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0093 - accuracy: 0.6632\n",
      "Epoch 168/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0086 - accuracy: 0.6638\n",
      "Epoch 169/200\n",
      "143/143 [==============================] - 27s 189ms/step - loss: 0.0067 - accuracy: 0.6652\n",
      "Epoch 170/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0081 - accuracy: 0.6630\n",
      "Epoch 171/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0072 - accuracy: 0.6611\n",
      "Epoch 172/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0075 - accuracy: 0.6651\n",
      "Epoch 173/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0078 - accuracy: 0.6650\n",
      "Epoch 174/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0078 - accuracy: 0.6625\n",
      "Epoch 175/200\n",
      "143/143 [==============================] - 27s 189ms/step - loss: 0.0065 - accuracy: 0.6662\n",
      "Epoch 176/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0076 - accuracy: 0.6663\n",
      "Epoch 177/200\n",
      "143/143 [==============================] - 27s 185ms/step - loss: 0.0078 - accuracy: 0.6631\n",
      "Epoch 178/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0069 - accuracy: 0.6612\n",
      "Epoch 179/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0082 - accuracy: 0.6630\n",
      "Epoch 180/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0072 - accuracy: 0.6641\n",
      "Epoch 181/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0070 - accuracy: 0.6621\n",
      "Epoch 182/200\n",
      "143/143 [==============================] - 26s 183ms/step - loss: 0.0075 - accuracy: 0.6629\n",
      "Epoch 183/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0053 - accuracy: 0.6607\n",
      "Epoch 184/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0056 - accuracy: 0.6658\n",
      "Epoch 185/200\n",
      "143/143 [==============================] - 27s 187ms/step - loss: 0.0076 - accuracy: 0.6644\n",
      "Epoch 186/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0067 - accuracy: 0.6614\n",
      "Epoch 187/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0077 - accuracy: 0.6628\n",
      "Epoch 188/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0055 - accuracy: 0.6670\n",
      "Epoch 189/200\n",
      "143/143 [==============================] - 27s 185ms/step - loss: 0.0055 - accuracy: 0.6614\n",
      "Epoch 190/200\n",
      "143/143 [==============================] - 26s 185ms/step - loss: 0.0057 - accuracy: 0.6630\n",
      "Epoch 191/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0080 - accuracy: 0.6632\n",
      "Epoch 192/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0061 - accuracy: 0.6641\n",
      "Epoch 193/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0065 - accuracy: 0.6628\n",
      "Epoch 194/200\n",
      "143/143 [==============================] - 26s 186ms/step - loss: 0.0055 - accuracy: 0.6643\n",
      "Epoch 195/200\n",
      "143/143 [==============================] - 27s 185ms/step - loss: 0.0065 - accuracy: 0.6643\n",
      "Epoch 196/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0068 - accuracy: 0.6641\n",
      "Epoch 197/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0085 - accuracy: 0.6630\n",
      "Epoch 198/200\n",
      "143/143 [==============================] - 27s 188ms/step - loss: 0.0051 - accuracy: 0.6611\n",
      "Epoch 199/200\n",
      "143/143 [==============================] - 26s 184ms/step - loss: 0.0068 - accuracy: 0.6646\n",
      "Epoch 200/200\n",
      "143/143 [==============================] - 27s 186ms/step - loss: 0.0043 - accuracy: 0.6672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbe42352bd0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "accomplished-sewing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 피곤해\n",
      "출력 : 눈 체조를 해보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'눈 체조를 해보세요 .'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('피곤해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "defined-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 졸려 죽겠어\n",
      "출력 : 친구를 사귀어 보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'친구를 사귀어 보세요 .'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('졸려 죽겠어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "animal-boston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 주말이 언제 올까?\n",
      "출력 : 사랑의 보통은 없죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'사랑의 보통은 없죠 .'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('주말이 언제 올까?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "neither-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 뭐먹지?\n",
      "출력 : 누구랑 가느냐가 중요하겠죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'누구랑 가느냐가 중요하겠죠 .'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘 뭐먹지?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-maintenance",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "처음 프로젝트 코드를 진행했을때 출력값이 마침표 하나만 나와서 당황했다. 그래서 불러온 쳇봇 데이터가 잘못되었나 생각되어 \n",
    "\n",
    "https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv 의 데이터를 다시 불러와보니 용량차이도 있어서 데이터를 잘못 불러온것을 확신 하였다. 그러나 결과가 비슷하였고 다시 확인해보니 전처리 과정을 빼먹고 진행하고 있었음을 알수있엇다. \n",
    "\n",
    "데이터두 바꾸고 전처리 과정두 진행하니 정상적으로 토크나이저를 통해서 단어장이 생성됨을 확인할 수 있었다.\n",
    "\n",
    "D_MODEL 을 256 ->512 로 변경해 보았으며, NUM_LAYERS 를 2-> 5로 변경시도 해보고 EPOCHS 도 50 -> 200 으로 올려 보았다.\n",
    "\n",
    "질문에 따라서 처음부터 매끄러운 답변을 확인할 수 있엇던 내용도 있었으며.. 여러 시도를 해도 동문서답 같은 답변을 주는 것도 확인할 수 있었다.\n",
    "\n",
    "그런데 생각보다 자연스러운 답을 받을수 있어서 처음 siri를 통해서 \"인생이란?\" 이라고 질문을 던졌을때 받아본 답변에 놀랐던 과거가 생각나면서 챗봇이 진화하면 충분히 사람과 원활한 대화를 나누면서도 상대방이 챗봇임을 눈치채지 못하는 경지에 이를수 있겟다는 생각이 들었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-population",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
